{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":106695,"databundleVersionId":12914564,"sourceType":"competition"},{"sourceId":12504717,"sourceType":"datasetVersion","datasetId":7892176},{"sourceId":165740,"sourceType":"modelInstanceVersion","modelInstanceId":141018,"modelId":163622}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi || echo \"No GPU available\"\n!pip show transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T07:27:17.383079Z","iopub.execute_input":"2025-07-18T07:27:17.383353Z","iopub.status.idle":"2025-07-18T07:27:19.565235Z","shell.execute_reply.started":"2025-07-18T07:27:17.383330Z","shell.execute_reply":"2025-07-18T07:27:19.564554Z"}},"outputs":[{"name":"stdout","text":"Fri Jul 18 07:27:17 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   50C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nName: transformers\nVersion: 4.52.4\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: kaggle-environments, peft, sentence-transformers\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nassert torch.cuda.is_available(), \"CUDA is not available\"\ndevice = torch.device(\"cuda:0\")\n\nprint(f\"Using device: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T07:27:20.272525Z","iopub.execute_input":"2025-07-18T07:27:20.273167Z","iopub.status.idle":"2025-07-18T07:27:22.009056Z","shell.execute_reply.started":"2025-07-18T07:27:20.273134Z","shell.execute_reply":"2025-07-18T07:27:22.008347Z"}},"outputs":[{"name":"stdout","text":"Using device: Tesla T4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -q \\\n    transformers==4.40.2 \\\n    peft==0.11.0 \\\n    accelerate==0.30.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T07:27:22.975991Z","iopub.execute_input":"2025-07-18T07:27:22.976322Z","iopub.status.idle":"2025-07-18T07:27:34.979095Z","shell.execute_reply.started":"2025-07-18T07:27:22.976303Z","shell.execute_reply":"2025-07-18T07:27:34.978402Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nfrom peft import PeftModel\nfrom datasets import load_dataset\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T07:27:49.755207Z","iopub.execute_input":"2025-07-18T07:27:49.755947Z","iopub.status.idle":"2025-07-18T07:27:53.091815Z","shell.execute_reply.started":"2025-07-18T07:27:49.755897Z","shell.execute_reply":"2025-07-18T07:27:53.091151Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"BASE_MODEL = \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2\"\nLORA_ADAPTER = \"/kaggle/input/final-results/lora_phi3_stage2/checkpoint-6500\"\nTEST_PATH = \"/kaggle/input/goedel-machines-x-iitm-clinical-llm-challenge\"\nOUT_CSV = \"/kaggle/working/submission.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T07:27:54.479077Z","iopub.execute_input":"2025-07-18T07:27:54.479976Z","iopub.status.idle":"2025-07-18T07:27:54.483674Z","shell.execute_reply.started":"2025-07-18T07:27:54.479950Z","shell.execute_reply":"2025-07-18T07:27:54.482899Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, local_files_only=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n    trust_remote_code=True,\n    local_files_only=True\n)\n\nmodel = PeftModel.from_pretrained(base_model, LORA_ADAPTER)\nmodel.eval()\nmodel.cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T07:27:57.287151Z","iopub.execute_input":"2025-07-18T07:27:57.287433Z","iopub.status.idle":"2025-07-18T07:28:06.088372Z","shell.execute_reply.started":"2025-07-18T07:27:57.287410Z","shell.execute_reply":"2025-07-18T07:28:06.087306Z"}},"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304d4307f8e44eac8e9abf34376ffbc7"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_128/125651380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLORA_ADAPTER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# load the config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0m\u001b[1;32m    372\u001b[0m                 PeftConfig._get_peft_type(\n\u001b[1;32m    373\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclass_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloaded_attributes\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_peft_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36mfrom_peft_type\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mconfig_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconfig_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: LoraConfig.__init__() got an unexpected keyword argument 'corda_config'"],"ename":"TypeError","evalue":"LoraConfig.__init__() got an unexpected keyword argument 'corda_config'","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\n\nfrom transformers import GenerationConfig\n\ngen_config = GenerationConfig.from_pretrained(BASE_MODEL)\ngen_config.max_new_tokens = 10\n\ndef predict_mcq(question, options):\n    prompt = f\"{question}\\n\" + \"\\n\".join([f\"({k}) {v}\" for k, v in options.items()]) + \"\\nAnswer:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    with torch.no_grad():\n        outputs = model.generate(**inputs, generation_config=gen_config)\n    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    for opt in options:\n        if f\"({opt})\" in decoded:\n            return opt\n    return \"A\" \n\nall_preds = []\nfor level in [\"easy\", \"medium\", \"hard\"]:\n    dataset = load_dataset(\n        \"json\", \n        data_files=f\"/kaggle/input/goedel-machines-x-iitm-clinical-llm-challenge/train_{level}.jsonl\", \n        split=\"train\"\n    )\n    for ex in dataset:\n        pred = predict_mcq(ex[\"question\"], ex[\"options\"])\n        all_preds.append({\"id\": ex[\"id\"], \"answer\": pred})\n\nsubmission_df = pd.DataFrame(all_preds)\nsubmission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"✅ Submission saved at /kaggle/working/submission.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(file_path):\n    dataset = load_dataset(\"json\", data_files=file_path, split=\"train\")\n    correct = 0\n    for example in dataset:\n        pred = predict_answer(example[\"question\"], example[\"options\"])\n        if pred == example[\"answer\"]:\n            correct += 1\n    accuracy = correct / len(dataset)\n    print(f\"{file_path} — Accuracy: {accuracy:.4f}\")\n    return accuracy\n\nprint(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\nacc_easy = evaluate(\"/kaggle/input/goedel-machines-x-iitm-clinical-llm-challenge/sample_test_easy.jsonl\")\nacc_medium = evaluate(\"/kaggle/input/goedel-machines-x-iitm-clinical-llm-challenge/sample_test_medium.jsonl\")\nacc_hard = evaluate(\"/kaggle/input/goedel-machines-x-iitm-clinical-llm-challenge/sample_test_hard.jsonl\")\n\noverall = 0.30 * acc_easy + 0.30 * acc_medium + 0.40 * acc_hard\nprint(f\"Overall Score: {overall:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}