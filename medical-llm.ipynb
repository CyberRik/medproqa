{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":106695,"databundleVersionId":12914564,"sourceType":"competition"},{"sourceId":165740,"sourceType":"modelInstanceVersion","modelInstanceId":141018,"modelId":163622}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi || echo \"No GPU available\"\n!pip show transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q peft accelerate bitsandbytes\n!pip install -q fsspec==2023.9.2\n!pip install -q -U datasets\n\nprint(\"All packages installed successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q peft accelerate bitsandbytes\n!pip install -q fsspec==2023.9.2\n!pip install -q -U datasets\n\nprint(\"All packages installed successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nassert torch.cuda.is_available(), \"CUDA GPU not available!\"\nassert torch.cuda.device_count() == 1, f\"Multiple GPUs detected: {torch.cuda.device_count()}\"\n\nprint(\"Using GPU:\", torch.cuda.get_device_name(0))\nprint(\"VRAM (total):\", round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2), \"GB\")\n\nprint(torch.cuda.memory_summary(device=0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading datasets\nfrom datasets import load_dataset\n\ntrain_easy = load_dataset(\"json\", data_files=\"/kaggle/input/goedel-machines-x-iitm-clinical-llm-challenge/train_easy.jsonl\", split=\"train\")\ntrain_medium = load_dataset(\"json\", data_files=\"/kaggle/input/goedel-machines-x-iitm-clinical-llm-challenge/train_medium.jsonl\", split=\"train\")\n\nprint(f\"train_easy: {len(train_easy)} questions\")\nprint(f\"train_medium: {len(train_medium)} questions\")\n\nprint(\"Sample from train_easy:\\n\", train_easy[0])\nprint(\"Sample from train_medium:\\n\", train_medium[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading the model and tokenizer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nfrom peft import prepare_model_for_kbit_training\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\nMODEL_PATH = \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    quantization_config=bnb_config,\n    device_map={\"\": 0},\n    trust_remote_code=True,\n    local_files_only=True\n)\n\nmodel = prepare_model_for_kbit_training(model)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#preprocessing\ndef preprocess_clinical_mcq(example):\n    question = example[\"question\"]\n    options = example[\"options\"]\n    answer_label = example[\"answer\"]\n\n    option_texts = [f\"({key}) {value}\" for key, value in options.items()]\n    prompt = f\"Question: {question}\\nChoices:\\n\" + \"\\n\".join(option_texts) + \"\\nAnswer: \"\n    full_text = prompt + answer_label\n\n    encoded = tokenizer(\n        full_text,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=256\n    )\n\n    labels = encoded[\"input_ids\"].copy()\n\n    prompt_len = len(tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=256)[\"input_ids\"])\n    labels[:prompt_len] = [-100] * prompt_len\n\n    return {\n        \"input_ids\": encoded[\"input_ids\"],\n        \"attention_mask\": encoded[\"attention_mask\"],\n        \"labels\": labels\n    }\n\nprint(\"preprocessing done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\ntrain_combined = concatenate_datasets([train_easy, train_medium])\ntrain_combined = train_combined.select(range(64000))\n\ntokenized_dataset = train_combined.map(\n    preprocess_clinical_mcq,\n    remove_columns=train_combined.column_names\n)\n\nsplit = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\ntrain_dataset = split[\"train\"]\neval_dataset = split[\"test\"]\n\nprint(f\"Train size: {len(train_dataset)}\")\nprint(f\"Eval size: {len(eval_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import EvalPrediction\nimport numpy as np\n\ndef decode_labels(preds, labels):\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return decoded_preds, decoded_labels\n\ndef compute_metrics(eval_pred: EvalPrediction):\n    predictions, labels = eval_pred\n    decoded_preds, decoded_labels = decode_labels(predictions, labels)\n\n    correct = 0\n    total = 0\n    for pred, label in zip(decoded_preds, decoded_labels):\n        pred_ans = pred.strip().split(\"Answer:\")[-1].strip().upper()[:1]\n        true_ans = label.strip().split(\"Answer:\")[-1].strip().upper()[:1]\n\n        if pred_ans == true_ans:\n            correct += 1\n        total += 1\n\n    accuracy = correct / total if total > 0 else 0\n    return {\"accuracy\": accuracy}\nprint(\"computed metrics\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lora\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"qkv_proj\", \"o_proj\"], \n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/lora_phi3_stage1\",  \n    per_device_train_batch_size=2,                    \n    gradient_accumulation_steps=8,                    \n    learning_rate=5e-5,                               \n    num_train_epochs=2,                               \n    logging_steps=500,         \n    save_steps=500,\n    save_total_limit=2,\n    save_strategy=\"steps\", \n    fp16=True,                                       \n    gradient_checkpointing=True,                     \n    remove_unused_columns=False,\n    report_to=\"none\"\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\nprint(\"training args defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model():\n    print(\"Training started...\")\n    try:\n        trainer.train()\n        print(\"Training complete.\")\n        trainer.save_model(\"/kaggle/working/lora_phi3_stage1\")\n        tokenizer.save_pretrained(\"/kaggle/working/lora_phi3_stage1\")\n        print(\"Model and tokenizer saved to /kaggle/working/lora_phi3_stage1\")\n    except Exception as e:\n        print(\"Training failed with error:\", str(e))\n\n# ⛔️ Don't use threading on Kaggle commit\ntrain_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/lora_phi3_stage1\", \"zip\", \"/kaggle/working/lora_phi3_stage1\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}